# MATH 6220: APPLIED FUNCTIONAL ANALYSIS (Spring 2020)

### Instructor: Alex Townsend, townsend@cornell.edu (Malott 589)
### TA: Max Hallgren, meh249@cornell.edu, Office hours: Wednesdays 4-6pm (Malott 218)

## ABSTRACT:
This applied functional analysis class emphasizes the modern uses of approximation theory, reproducing kernel 
Hilbert spaces, and rational functions. We will cover a selection of theorems in functional analysis and how they
are having real-life consequences for modern computations.  From a functional analysis perspective, we will 
answer questions such as 
- Peetre’s Theorem: “Why are differential equations so ubiquitous in physics and applied mathematics?”, 
- Mairhuber-Curtis Theorem: “Why do so many ideas in 1D not extend to 2D for data-fitting?” 
- Representer Theorem: “How ReLU for deep neural nets is related to an infinite-dimensional optimization problem?” 
- AAK Theory: “Why are eigenvalues and rationals unlikely buddies?”  
Warning: For those students looking for a theoretical side of functional analysis, see MATH 7130. Warning: The syllabus is very different from MATH 6220 in Spring 2018 and Spring 2019. 

## LOCATION AND TIME: 
8:40am - 9:55am, Malott Hall 251, Tuesdays and Thursdays

## TEXTBOOKS AND RESOURCES:
I have tried to select freely available textbooks and resources as much as possible. 
- ATAP = “Approximation theory and approximation practice” by Nick Trefethen. I will be using the second edition, 
but the 1st edition is also fine. [First 6 chapters, freely available]
- RKHS = “Introduction to the theory of reproducing kernel Hilbert spaces” by Vern Paulsen and Mrinal 
Raghupathi. [freely available to Cornell students]
- HANKEL = “An Excursion into the Theory of Hankel Operators” by Vladimir Peller. [freely available]
- UNSER = “A representer theorem for deep neural networks” by Michael Unser. [freely available]

## LECTURES:
### Part I: Approximation theory and Approximation practice
- L1: Proxy for functions: Chebyshev polynomials and series (ATAP, Chap 2 & 3)
- L2: Interpolants, truncations, and barycentric interpolation formula (ATAP, Chap 4 & 5)
- L3: Weierstrass approximation theorem, convergence for diff (ATAP, Chap 6 & 7)
- L4: When to use the L1, L2, and Linf norm in computations.
- L5: Quadrature rules and rational functions (ATAP, Chap 19)

### Part II: Reproducing Kernel Hilbert spaces
- L6: Reproducing Kernel Hilbert spaces (RKHS, 1.1, 1.2)
- L7: Positive definite kernels (RKHS 2.2, 2.3)
- L8: High-dimensional scattered data approximation (RKHS 3.1)
- L9: Regression problems and the kernel method (RKHS 8.1, 8.2)
- L10: The representer theorem (RKHS 8.6)
- L11: The representer theorem in deep learning (UNSER)

### Part III: Integral and differential operators
- L12: Integral operators and Schwarz kernel theorem (RKHS 11.1)
- L13: Mercer’s theorem and the SVD (RKHS 11.3)
- L14: Weyl’s theorem, Cauchy’s interlacing theorem 
- L15: Analogues of matrix factorizations (RKHS 4.1)
- L16: Differential operators and Peetre’s theorem (no notes yet)
- L17: Peetre’s theorem (no notes yet)
- L18: Krylov methods for differential operators (no notes yet)

### Part IV: Rational functions
- L19: Nonlinear approximation: why rational functions? Best approximation. (ATAP, Chap 23)
- L20: Two famous problems, rational interpolation and linearized least squares (ATAP, Chap 25)
- L21: AAA method, analytic continuation and convergence acceleration. (ATAP, Chap 28)
- L22: Hardy spaces, Hankel operators, and AAK theory (HANKEL)
- L23: AAK theory (HANKEL)

### Part V: Theoretical aspects of linear algebra and machine learning 
- L24: Robert Gower (guest lecturer)
- L25: Robert Gower (guest lecturer)
- L26: Robert Gower (guest lecturer)
